## Probability Spaces

### Introduction

A probability space provides the formal mathematical foundation for modeling random phenomena. This structure enables rigorous reasoning about uncertainty by defining a consistent framework for events, outcomes, and their associated probabilities. This chapter presents formal definitions, essential properties, and foundational results underlying modern probability theory.

### Sigma-Algebras and Measure-Theoretic Preliminaries

Let $\Omega$ denote a nonempty set called the **sample space**, representing all possible outcomes of a random experiment. A **sigma-algebra** $\mathcal{F}$ on $\Omega$ is a collection of subsets of $\Omega$ (called **events**) satisfying:

1. $\Omega \in \mathcal{F}$,
2. If $A \in \mathcal{F}$, then $A^{c} \in \mathcal{F}$,
3. If ${A_n}*{n=1}^\infty \subset \mathcal{F}$, then $\bigcup*{n=1}^\infty A_n \in \mathcal{F}$.

Closure under countable intersections follows from De Morgan's laws and properties 2–3.

### Probability Space (Formal Definition)

A **probability space** is a triple $(\Omega, \mathcal{F}, P)$, where:

* $\Omega$ is the sample space,
* $\mathcal{F}$ is a sigma-algebra of subsets of $\Omega$,
* $P: \mathcal{F} \to [0, 1]$ is a **probability measure**, satisfying:

  1. $P(\Omega) = 1$,
  2. $P(\emptyset) = 0$,
  3. $P(A) \geq 0$ for all $A \in \mathcal{F}$,
  4. For any countable sequence of pairwise disjoint events ${A_n}_{n=1}^{\infty} \subset \mathcal{F}$,

  $$
  P\left(\bigcup_{n=1}^{\infty} A_n\right) = \sum_{n=1}^{\infty} P(A_n).
  $$

This axiomatic system is foundational and generalizes intuitive probability assignments to complex, possibly uncountable, spaces [@Billingsley1995].

### Random Variables

A **random variable** is a measurable function $X: (\Omega, \mathcal{F}) \to (\mathbb{R}, \mathcal{B}(\mathbb{R}))$, where $\mathcal{B}(\mathbb{R})$ denotes the Borel sigma-algebra of $\mathbb{R}$. That is, for every Borel set $B \subseteq \mathbb{R}$,

$$
X^{-1}(B) \in \mathcal{F}.
$$

This property (measurability) ensures that the probability of any event concerning $X$ is well-defined [@Durrett2019].

#### Measurability and Pre-Image

A function $X$ is **measurable** if, for every Borel set $B$, the pre-image $X^{-1}(B) = {\omega \in \Omega : X(\omega) \in B}$ belongs to $\mathcal{F}$.

### Probability Distributions of Random Variables

The **distribution** of a random variable $X$ is defined as the pushforward measure $P_X(B) = P(X^{-1}(B))$ for $B \in \mathcal{B}(\mathbb{R})$.

#### Cumulative Distribution Function (CDF)

The **cumulative distribution function** of $X$ is

$$
F_X(x) = P(X \leq x), \quad x \in \mathbb{R}.
$$

#### Discrete and Continuous Distributions

* **Discrete:** If $X$ takes at most countably many values, define the **probability mass function (pmf)**

  $$
  p_X(x) = P(X = x), \quad \sum_{x \in \operatorname{Range}(X)} p_X(x) = 1.
  $$
* **Continuous:** If there exists a function $f_X$ such that

  $$
  P(X \in B) = \int_B f_X(x)\,dx
  $$

  for all Borel sets $B$, then $f_X$ is the **probability density function (pdf)** of $X$. It must satisfy $f_X(x) \geq 0$ and

  $$
  \int_{-\infty}^{\infty} f_X(x)\,dx = 1.
  $$

#### Mixed Distributions

Random variables may also have **mixed distributions** that are neither purely discrete nor purely continuous. Such cases arise, for example, in distributions with atoms and an absolutely continuous component [@Billingsley1995].

### Mathematical Expectation

Let $X$ be a random variable on $(\Omega, \mathcal{F}, P)$.

* The **mathematical expectation** (expected value) is defined as

  $$
  E[X] =
  \begin{cases}
  \sum_{x \in \operatorname{Range}(X)} x\,p_X(x), & \text{if $X$ is discrete,} \\
  \int_{-\infty}^{\infty} x\,f_X(x)\,dx, & \text{if $X$ is continuous.}
  \end{cases}
  $$

  provided $E[|X|] < \infty$.

#### Linearity and Monotone Convergence

Expectation is linear: For integrable $X, Y$ and constants $a, b$,

$$
E[aX + bY] = a E[X] + b E[Y].
$$

If $0 \leq X_n \uparrow X$, then $E[X_n] \uparrow E[X]$ (Monotone Convergence Theorem) [@Durrett2019].

### Variance and Standard Deviation

The **variance** of $X$ (with $E[X^2] < \infty$) is

$$
\operatorname{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2.
$$

The **standard deviation** is $\sigma_X = \sqrt{\operatorname{Var}(X)}$.

### Conditional Probability

For $A, B \in \mathcal{F}$ with $P(B) > 0$, the **conditional probability** of $A$ given $B$ is

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}.
$$

### Fundamental Theorems

#### Law of Total Probability

Let ${A_i}_{i=1}^{n}$ be a finite or countable partition of $\Omega$ with $P(A_i) > 0$ for all $i$. For any $B \in \mathcal{F}$,

$$
P(B) = \sum_{i} P(B|A_i)P(A_i).
$$

**Proof.**
Since the $A_i$ are disjoint and $\bigcup_{i} A_i = \Omega$,

$$
P(B) = P\left(B \cap \Omega\right) = P\left(B \cap \bigcup_{i} A_i\right) = \sum_{i} P(B \cap A_i) = \sum_{i} P(B|A_i)P(A_i).
$$

#### Bayes’ Theorem

For events $A, B \in \mathcal{F}$ with $P(B) > 0$,

$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
$$

**Proof.**
By the definition of conditional probability,

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}.
$$

### Illustrative Example: Discrete and Continuous Random Variables

#### Discrete Example: Sum of Two Dice

Let $\Omega = {(i,j) : i, j \in {1,\ldots,6}}$ and define $X(i,j) = i + j$. Then $\operatorname{Range}(X) = {2,3,\ldots,12}$.

For $x \in \operatorname{Range}(X)$,

$$
p_X(x) = P(X = x) = \frac{\#\text{ of pairs with sum } x}{36}.
$$

For $x = 7$, there are 6 pairs; so $p_X(7) = 6/36 = 1/6$.

Compute expectation:

$$
E[X] = \sum_{x=2}^{12} x \cdot p_X(x) = 7.
$$

Compute variance:

$$
\operatorname{Var}(X) = \sum_{x=2}^{12} (x-7)^2 p_X(x) = \frac{35}{6}.
$$

#### Continuous Example: Exponential Distribution

Let $X$ have pdf $f_X(x) = \lambda e^{-\lambda x}$ for $x \geq 0$ ($\lambda > 0$).

* $F_X(x) = 1 - e^{-\lambda x}$ for $x \geq 0$,
* $E[X] = \int_{0}^{\infty} x \lambda e^{-\lambda x} dx = 1/\lambda$,
* $\operatorname{Var}(X) = \int_{0}^{\infty} (x - 1/\lambda)^2 \lambda e^{-\lambda x} dx = 1/\lambda^2$.

### Exercises

1. **Probability Space Construction:**
   Given $\Omega = [0,1]$, let $\mathcal{F}$ be the Borel sigma-algebra, and define $P$ as the Lebesgue measure. Prove that $(\Omega, \mathcal{F}, P)$ is a probability space.

2. **Sigma-Algebra Verification:**
   Prove that the intersection of a countable collection of sigma-algebras on $\Omega$ is itself a sigma-algebra.

3. **Theorem Application:**
   For mutually exclusive $A_1, A_2, \ldots, A_n$ with $\sum_{i=1}^{n} P(A_i) = 1$ and an event $B$ with $P(B) > 0$, derive $P(A_j|B)$ using Bayes' theorem.

4. **Advanced Calculation:**
   Let $X \sim \text{Exp}(\lambda)$. Prove $E[X^k] = k!/\lambda^k$ for integer $k \geq 1$.

