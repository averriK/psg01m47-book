# Variables Aleatorias

## Definición y Propiedades

Sea $(\Omega, \mathcal{F}, P)$ un espacio de probabilidad, donde $\Omega$ es el espacio muestral, $\mathcal{F}$ es una $\sigma$-álgebra de subconjuntos de $\Omega$, y $P$ es una medida de probabilidad en $\mathcal{F}$.
Una **variable aleatoria** es una función medible $X: \Omega \to \mathbb{R}$, lo que significa que para todo $x \in \mathbb{R}$, el conjunto ${\omega \in \Omega : X(\omega) \leq x}$ pertenece a $\mathcal{F}$ [@Billingsley1995].

La **función de distribución acumulativa (CDF)** de $X$ se define por

$$
F_X(x) = P(X \leq x), \quad x \in \mathbb{R}.
$$

### Propiedades de la CDF

1. $F_X$ es monótonamente no decreciente: Si $x_1 < x_2$, entonces $F_X(x_1) \leq F_X(x_2)$.
2. $F_X$ es continua por la derecha: $\lim_{h \to 0^+} F_X(x+h) = F_X(x)$ para todo $x$.
3. Límites: $\lim_{x \to -\infty} F_X(x) = 0$ y $\lim_{x \to \infty} F_X(x) = 1$.

**Demostración**:
La monotonicidad y la continuidad por la derecha se siguen directamente de la definición de la medida de probabilidad y las propiedades de las $\sigma$-álgebras [@Billingsley1995, p.14].
Los límites en $\pm\infty$ se siguen de la aditividad numerable de $P$ y el agotamiento de la recta real.

## Variables Aleatorias Discretas

Una variable aleatoria $X$ es **discreta** si su rango es un conjunto finito o numerable $S \subset \mathbb{R}$. La **función de masa de probabilidad (pmf)** es

$$
p_X(x) = P(X = x), \quad x \in S
$$

con $\sum_{x \in S} p_X(x) = 1$.

**Ejemplo: Suma de Dos Dados Justos:** Sea $X$ la suma de los resultados de lanzar dos dados justos independientes. Los valores posibles son $x \in {2, 3, \dots, 12}$.
Cada resultado $(i, j)$ con $i, j \in {1,\dots,6}$ es igualmente probable. El número de pares $(i,j)$ que dan suma $x$ es $n(x) = 6 - |7 - x|$, por lo que

$$
p_X(x) = \frac{n(x)}{36}, \quad n(x) = 6 - |7 - x|, \quad x \in {2,3,\dots,12}.
$$

**Derivación**: Para $x=7$, $n(7)=6$; para $x=2$, $n(2)=1$; etc.

## Variables Aleatorias Continuas

Una variable aleatoria $X$ es **continua** si existe una **función de densidad de probabilidad (pdf)** $f_X:\mathbb{R}\to[0,\infty)$ tal que, para todo $a < b$,

$$
P(a < X \leq b) = \int_a^b f_X(x)\,dx.
$$

La pdf satisface

1. $f_X(x) \geq 0$ para todo $x \in \mathbb{R}$
2. $\int_{-\infty}^{\infty} f_X(x)\,dx = 1$

**Observación**: La existencia de una pdf requiere que $F_X$ sea absolutamente continua [@Durrett2019, Sec. 1.3].

**Ejemplo: Distribución Normal Estándar:** Sea $X$ con distribución normal estándar:

$$
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}, \quad x \in \mathbb{R}
$$

Entonces, para cualquier $a < b$,

$$
P(a < X \leq b) = \int_a^b f_X(x)\,dx.
$$

## Transformaciones de Variables Aleatorias

Dada una variable aleatoria $X$ con pdf $f_X$ y una función $h:\mathbb{R}\to\mathbb{R}$ medible, invertible y diferenciable, definamos $Y = h(X)$. La pdf de $Y$ es

$$
f_Y(y) = f_X(h^{-1}(y)) \left| \frac{d}{dy} h^{-1}(y) \right|, \quad y \in h(\mathbb{R})
$$

**Supuestos**: $h$ es estrictamente monótona y diferenciable con inversa $h^{-1}$, y $f_X$ es conocida.

** Ejemplo: Normal Estándar al Cuadrado:** Sea $X \sim N(0,1)$, $Y = X^2$. Entonces para $y \geq 0$,

* $h^{-1}(y) = \pm \sqrt{y}$,
* $f_X(h^{-1}(y)) = \frac{1}{\sqrt{2\pi}} e^{-y/2}$ para ambas raíces,
* $\left|\frac{d}{dy} h^{-1}(y)\right| = \frac{1}{2\sqrt{y}}$.

Por tanto,

$$
f_Y(y) = f_X(\sqrt{y})\frac{1}{2\sqrt{y}} + f_X(-\sqrt{y})\frac{1}{2\sqrt{y}} = \frac{1}{\sqrt{2\pi y}} e^{-y/2}, \quad y > 0,
$$

que es la distribución chi-cuadrada con un grado de libertad [@Billingsley1995, Sec. 10].

## Vectores Aleatorios

Un **vector aleatorio** $\mathbf{X} = (X_1, \dots, X_n)$ es una función medible $\mathbf{X}:\Omega \to \mathbb{R}^n$. La **función de distribución conjunta** es

$$
F_{\mathbf{X}}(x_1, \dots, x_n) = P(X_1 \leq x_1, \dots, X_n \leq x_n)
$$

Si existe una pdf conjunta $f_{\mathbf{X}}$, entonces

$$
P((X_1, \dots, X_n) \in B) = \int_{B} f_{\mathbf{X}}(x_1, \dots, x_n)\,dx_1 \cdots dx_n
$$

para todos los conjuntos de Borel $B \subset \mathbb{R}^n$.

**Marginales y Condicionales**: La pdf marginal de $X_1$ es

$$
f_{X_1}(x_1) = \int_{\mathbb{R}^{n-1}} f_{\mathbf{X}}(x_1, x_2, \dots, x_n)\,dx_2 \cdots dx_n
$$

Las distribuciones condicionales se definen de manera análoga [@Durrett2019, Sec. 1.5].

**Ejemplo: Normal Bivariada.** Sea $(X_1, X_2)$ conjuntamente normal con media cero, varianzas $1$, y covarianza $\rho$. La pdf conjunta es

$$
f_{\mathbf{X}}(x_1, x_2) = \frac{1}{2\pi\sqrt{1-\rho^2}} \exp\left( -\frac{x_1^2 - 2\rho x_1 x_2 + x_2^2}{2(1-\rho^2)} \right)
$$

## Independencia de Variables Aleatorias

Las variables aleatorias $X$ e $Y$ son **independientes** si, para todo $x,y \in \mathbb{R}$,

$$
P(X \leq x, Y \leq y) = P(X \leq x) P(Y \leq y)
$$

Equivalentemente, $F_{(X,Y)}(x,y) = F_X(x) F_Y(y)$ para todo $x, y$.

**Criterios Equivalentes**: La independencia se cumple si y solo si $P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$ para todos los conjuntos de Borel $A, B \subseteq \mathbb{R}$ [@Billingsley1995, Thm. 12.1].

**Contraejemplo**: Sea $X$ uniforme en $[-1,1]$, $Y=X$. $P(X \leq x, Y \leq y) = P(X \leq \min{x, y})$ no es igual a $P(X \leq x)P(Y \leq y)$ a menos que $x$ o $y$ sea $-\infty$ o $\infty$.

## Esperanza y Varianza

Sea $X$ una variable aleatoria.

* **Esperanza**:

  * Discreta: $E[X] = \sum_{x} x p_X(x)$, siempre que $\sum_x |x| p_X(x) < \infty$.
  * Continua: $E[X] = \int_{-\infty}^\infty x f_X(x)\,dx$ si $\int |x| f_X(x)\,dx < \infty$.

* **Varianza**:

$$
\operatorname{Var}(X) = E\left[(X - E[X])^2\right] = E[X^2] - (E[X])^2
$$

**Propiedades**: Para cualquier $a,b \in \mathbb{R}$,

* $E[aX + b] = aE[X] + b$
* $\operatorname{Var}(aX + b) = a^2 \operatorname{Var}(X)$

**Demostraciones**: Ver [@Durrett2019, Prop. 1.5.1].

## Simulación de Variables Aleatorias

El **método de transformación inversa** es una técnica fundamental de simulación:
Si $U \sim \text{Uniforme}(0,1)$ y $F_X$ es continua y estrictamente creciente, entonces $X = F_X^{-1}(U)$ tiene función de distribución $F_X$.

**Simulación avanzada**:

* **Método de aceptación-rechazo**: Generar $Y$ con densidad $g$, aceptar con probabilidad $f_X(Y)/[c g(Y)]$, donde $c$ satisface $f_X(y) \leq c g(y)$ para todo $y$ [@Devroye1986].
* **Método de Box–Muller**: Para $X$ normal estándar, generar $U_1, U_2$ independientes, uniformes en $(0,1)$. Entonces $X = \sqrt{-2\log U_1} \cos(2\pi U_2)$.

## Ejercicios

1. Demostrar que para cualquier CDF $F$, la función es continua por la derecha y satisface los límites establecidos en $\pm\infty$.
2. Sea $X$ uniforme en $[0,1]$, y $Y = -\ln(1-X)$. Mostrar que $Y$ es exponencial con parámetro $1$.
3. Dadas las variables aleatorias $X$ e $Y$ con pdf conjunta $f(x,y) = 6xy$ para $0 < x < 1$, $0 < y < 1$, $x+y < 1$, calcular $P(X < Y)$ y verificar si $X$ e $Y$ son independientes.
