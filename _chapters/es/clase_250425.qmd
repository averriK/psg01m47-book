## El Proceso de Bernoulli: Definición, Propiedades y Aplicaciones

### Definición Formal de Procesos Estocásticos

Sea $(\Omega, \mathcal{F}, P)$ un espacio de probabilidad, donde $\Omega$ es el espacio muestral, $\mathcal{F}$ es una $\sigma$-álgebra de eventos, y $P$ es una medida de probabilidad. Un **proceso estocástico** es una colección ${X_t : t \in T}$ de variables aleatorias $X_t: \Omega \to S$, indexadas por un conjunto $T$ (frecuentemente $\mathbb{N}$ o $\mathbb{R}_+$), donde $S$ es un espacio medible. Para cada $t \in T$ fijo, $X_t$ es una función medible, y para cada $\omega \in \Omega$ fijo, el mapeo $t \mapsto X_t(\omega)$ se llama una *trayectoria muestral* o *trayectoria* del proceso [@Billingsley1995; @Durrett2019].

### El Proceso de Bernoulli

Un **proceso de Bernoulli** es un proceso estocástico prototípico de tiempo discreto que modela secuencias de ensayos binarios independientes (éxito/fracaso).

#### Construcción Rigurosa

Sea $(\Omega, \mathcal{F}, P)$ un espacio de probabilidad. Un proceso de Bernoulli con parámetro $p \in (0,1)$ es una secuencia infinita de variables aleatorias independientes e idénticamente distribuidas ${X_n}_{n \in \mathbb{N}}$, donde para cada $n$,

$$
P(X_n = 1) = p, \qquad P(X_n = 0) = 1-p,
$$

y todas las $X_n$ son independientes.

* **Espacio muestral**: $\Omega = {0,1}^{\mathbb{N}}$, cada $\omega \in \Omega$ es una secuencia binaria infinita.
* **Sigma-álgebra**: $\mathcal{F}$ es la $\sigma$-álgebra producto en $\Omega$.
* **Medida de probabilidad**: $P$ es la medida producto que asigna $P(X_n=1)=p$, $P(X_n=0)=1-p$ independientemente para todo $n$.

#### Distribución Conjunta

Para cualquier $n\in\mathbb{N}$ y cualquier $(x_1,\ldots,x_n)\in{0,1}^n$,

$$
P(X_1=x_1,\ldots,X_n=x_n) = p^{\sum_{i=1}^n x_i}(1-p)^{n-\sum_{i=1}^n x_i}.
$$

**Demostración**: Por independencia y distribución idéntica, $P(X_i = x_i) = p$ si $x_i=1$, $1-p$ si $x_i=0$; el producto sobre $i$ produce el resultado [@Durrett2019].

### Variables Aleatorias Asociadas

#### Número Total de Éxitos en $n$ Ensayos

Definamos

$$
S_n = \sum_{i=1}^n X_i.
$$

Entonces $S_n$ es el número total de éxitos en los primeros $n$ ensayos.

**Teorema:** $S_n \sim \operatorname{Binomial}(n,p)$.

**Demostración:** $S_n$ es la suma de $n$ variables aleatorias Bernoulli$(p)$ independientes. Para $k=0,1,\ldots,n$,

$$
P(S_n = k) = \binom{n}{k} p^k (1-p)^{n-k}.
$$

Esto se sigue de la enumeración combinatoria de secuencias con $k$ éxitos y $n-k$ fracasos [@Billingsley1995].

#### Tiempo de Espera para el Primer Éxito

Sea

$$
T = \min \{ n \geq 1 : X_n = 1 \}
$$

el tiempo de espera hasta el primer éxito.

**Proposición:** $T$ es una variable aleatoria geométrica con parámetro $p$:

$$
P(T = n) = (1-p)^{n-1}p, \quad n=1,2,\ldots
$$

**Demostración:** Los primeros $n-1$ ensayos son fracasos ($X_1=0,\ldots,X_{n-1}=0$), el $n$-ésimo ensayo es un éxito ($X_n=1$). Por independencia,

$$
P(T = n) = (1-p)^{n-1}p.
$$

[@Durrett2019]

**Propiedad de Falta de Memoria:** Para $m, n \in \mathbb{N}$,

$$
P(T > m + n \mid T > n) = P(T > m).
$$

**Demostración:** Por la definición de independencia,

$$
P(T > n) = (1-p)^n.
$$

Por tanto,

$$
P(T > m + n \mid T > n) = \frac{P(T > m + n)}{P(T > n)} = \frac{(1-p)^{m+n}}{(1-p)^n} = (1-p)^m = P(T > m).
$$

#### Tiempo de Espera para el $k$-ésimo Éxito

Sea $T_k$ el ensayo en el que ocurre el $k$-ésimo éxito.

**Teorema:** $T_k$ sigue la distribución binomial negativa:

$$
P(T_k = t) = \binom{t-1}{k-1} p^k (1-p)^{t-k}, \quad t = k, k+1, \ldots
$$

**Demostración:** Esta es la probabilidad de que en $t-1$ ensayos, haya exactamente $k-1$ éxitos (en cualquier orden), y el $t$-ésimo ensayo sea un éxito. Ver [@Billingsley1995].

Además, $T_k$ es la suma de $k$ variables aleatorias geométrica$(p)$ independientes [@Durrett2019].

### Propiedades Clave

* **Independencia:** La secuencia ${X_n}$ es independiente por construcción.
* **Estacionaridad del Proceso:** Los incrementos $S_{n+m} - S_n$ son binomiales con parámetros $(m,p)$ y son independientes de $S_n$, reflejando la falta de memoria en el proceso.
* **Propiedad de Falta de Memoria:** El tiempo de espera geométrico $T$ para el primer éxito satisface $P(T > m + n \mid T > n) = P(T > m)$.
* **Terminología Estándar:** La propiedad se llama universalmente la *propiedad de falta de memoria*, no "memoria corta" [@Durrett2019].

### Conexiones con Otras Distribuciones

#### Aproximación de Poisson a la Binomial

Cuando $n$ es grande y $p$ es pequeño con $\lambda = np$ fijo, la distribución binomial se aproxima a la distribución de Poisson:

$$
\lim_{n \to \infty,\,p \to 0,\,np=\lambda} \binom{n}{k} p^k (1-p)^{n-k} = \frac{e^{-\lambda} \lambda^k}{k!}.
$$

Esta es la *ley de eventos raros* [@Billingsley1995, §19].

**Nota:** Las cotas de error y tasas de convergencia se pueden encontrar en [@LeCam1960].

#### Generalización Multinomial

Si cada ensayo admite $r>2$ resultados posibles con probabilidades constantes $p_1, \ldots, p_r$, el vector $(N_1,\ldots,N_r)$, donde $N_i$ cuenta las ocurrencias del resultado $i$ en $n$ ensayos, sigue la distribución multinomial:

$$
P(N_1 = n_1, \ldots, N_r = n_r) = \frac{n!}{n_1! \cdots n_r!} p_1^{n_1} \cdots p_r^{n_r}, \qquad \sum_{i=1}^r n_i = n.
$$

Esta es una generalización directa del modelo binomial [@Durrett2019].

**Ejemplo:** Supongamos que una línea de producción produce artículos con probabilidad de defecto independiente $p=0.02$. Si se verifican $n=100$ artículos:

* La probabilidad de exactamente $k=3$ artículos defectuosos es

  $$
  P(S_{100} = 3) = \binom{100}{3} (0.02)^3 (0.98)^{97} \approx 0.180.
  $$
* La probabilidad de que el primer defecto aparezca en el artículo 15:

  $$
  P(T = 15) = (0.98)^{14} (0.02) \approx 0.0148.
  $$

### Ejercicios

1. (**Construcción Rigurosa**) Construir explícitamente un espacio de probabilidad y definir las variables aleatorias coordenadas para modelar un proceso de Bernoulli con parámetro $p$.
2. (**Binomial Negativa**) Demostrar que la suma de $k$ variables aleatorias geométrica$(p)$ independientes tiene la distribución binomial negativa con parámetros $(k, p)$.
3. (**Aproximación de Poisson**) Para $n=500$, $p=0.006$, calcular $P(S_{n}=3)$ exactamente y aproximar con Poisson; discutir la precisión.