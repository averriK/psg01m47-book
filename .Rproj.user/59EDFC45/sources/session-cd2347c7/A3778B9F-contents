## Conditional Probability and Independence

### Conditional Probability

Let $(\Omega, \mathcal{A}, P)$ be a probability space. For events $A, B \in \mathcal{A}$ with $P(A) > 0$, the **conditional probability** of $B$ given $A$ is defined by

$$
P(B \mid A) = \frac{P(B \cap A)}{P(A)}.
$$

This formula expresses how the probability of $B$ changes when restricted to the occurrence of $A$.

#### Extension: Conditional Probability with Respect to a $\sigma$-Algebra

Given a sub-$\sigma$-algebra $\mathcal{G} \subseteq \mathcal{A}$, the **conditional probability of $B$ given $\mathcal{G}$** is a $\mathcal{G}$-measurable random variable $P(B \mid \mathcal{G})$ such that, for all $G \in \mathcal{G}$,

$$
\int_G P(B \mid \mathcal{G}) \, dP = P(B \cap G).
$$

This formulation generalizes conditional probability to settings involving random variables and information structures [ @Billingsley1995 ].

#### Conditional Probability Measure

For fixed $A$ with $P(A) > 0$, define the **conditional probability measure** $P_A(\cdot)$ on $(\Omega, \mathcal{A})$ by

$$
P_A(B) = P(B \mid A).
$$

$P_A$ satisfies:

* Non-negativity: $P_A(B) \geq 0$ for all $B \in \mathcal{A}$,
* Normalization: $P_A(\Omega) = 1$,
* Countable additivity: $P_A\left( \bigcup_{n=1}^\infty B_n \right) = \sum_{n=1}^\infty P_A(B_n)$ for any sequence of disjoint sets $B_n \in \mathcal{A}$.

##### Existence (Radon–Nikodym Theorem)

For a probability measure $P$ and $A \in \mathcal{A}$ with $P(A) > 0$, the Radon–Nikodym theorem ensures the existence of a conditional probability measure and, more generally, conditional expectations given a $\sigma$-algebra [ @Durrett2019 ].

#### Proof: Conditional Probability Properties

*Non-negativity* and *normalization* follow directly from the definition. *Countable additivity* is verified:
Let $(B_n)$ be disjoint. Then

$$
P\left( \bigcup_{n=1}^\infty B_n \mid A \right) = \frac{P\left( \bigcup_{n=1}^\infty (B_n \cap A) \right)}{P(A)} = \frac{\sum_{n=1}^\infty P(B_n \cap A)}{P(A)} = \sum_{n=1}^\infty \frac{P(B_n \cap A)}{P(A)} = \sum_{n=1}^\infty P(B_n \mid A).
$$

### Law of Total Probability

Let ${A_i}*{i=1}^n$ be a partition of $\Omega$ (i.e., $A_i \in \mathcal{A}$, $A_i \cap A_j = \emptyset$ for $i \ne j$, and $\bigcup*{i=1}^n A_i = \Omega$) with $P(A_i) > 0$ for all $i$. For any $B \in \mathcal{A}$,

$$
P(B) = \sum_{i=1}^n P(B \mid A_i) P(A_i).
$$

#### Proof

By the definition of a partition,

$$
P(B) = P\left( B \cap \Omega \right) = P\left( B \cap \bigcup_{i=1}^n A_i \right) = \sum_{i=1}^n P(B \cap A_i) = \sum_{i=1}^n P(B \mid A_i) P(A_i).
$$

### Bayes’ Theorem

For ${A_i}_{i=1}^n$ a partition of $\Omega$ with $P(A_i) > 0$, and $B$ such that $P(B) > 0$,

$$
P(A_j \mid B) = \frac{P(B \mid A_j) P(A_j)}{\sum_{i=1}^n P(B \mid A_i) P(A_i)}.
$$

#### Proof

By the definition of conditional probability and the law of total probability,

$$
P(A_j \mid B) = \frac{P(A_j \cap B)}{P(B)} = \frac{P(B \mid A_j) P(A_j)}{\sum_{i=1}^n P(B \mid A_i) P(A_i)}.
$$

#### Bayesian Interpretation

Bayes’ theorem enables the update from a **prior probability** $P(A_j)$ to a **posterior probability** $P(A_j \mid B)$ after observing $B$. In Bayesian statistics, this mechanism underpins statistical inference, with **conjugate priors** often used to ensure analytic tractability [ @BernardoSmith1994 ].

### Independence of Events and Random Variables

#### Independence of Events

Events $A_1, \dots, A_n$ are **mutually independent** if for every subset ${i_1, \dots, i_k} \subseteq {1,\dots, n}$,

$$
P\left( \bigcap_{j=1}^k A_{i_j} \right) = \prod_{j=1}^k P(A_{i_j}).
$$

**Pairwise independence** means $P(A_i \cap A_j) = P(A_i) P(A_j)$ for all $i \neq j$, but this does **not** guarantee mutual independence.

#### Independence of Random Variables

Random variables $X_1, \dots, X_n$ are **independent** if for all Borel sets $B_1, \dots, B_n \subseteq \mathbb{R}$,

$$
P\left( X_1 \in B_1, \dots, X_n \in B_n \right) = \prod_{j=1}^n P(X_j \in B_j).
$$

#### Independence with Respect to $\sigma$-Algebras

Sub-$\sigma$-algebras $\mathcal{G}_1, \dots, \mathcal{G}_n \subseteq \mathcal{A}$ are **independent** if for all $G_j \in \mathcal{G}_j$,

$$
P\left( \bigcap_{j=1}^n G_j \right) = \prod_{j=1}^n P(G_j).
$$

#### Example: Pairwise but Not Mutual Independence

Let $(\Omega, \mathcal{A}, P)$ be the space generated by three independent fair coin tosses. Define the following events:

* $A$: First toss is heads.
* $B$: Second toss is heads.
* $C$: The number of heads is even.

Each pair $(A,B)$, $(A,C)$, and $(B,C)$ is independent, but $A, B, C$ are not mutually independent.

#### Advanced Example: Independence in a Continuous Probability Space

Let $(\Omega, \mathcal{A}, P)$ be $([0,1], \mathcal{B}, \lambda)$, where $\lambda$ is the Lebesgue measure. Define events:

* $A = [0.1, 0.2)$ (first decimal digit is 1).
* $B = \bigcup_{k=0}^8 [0.01 + 0.1k, 0.02 + 0.1k)$ (second decimal digit is 1).

Compute:

* $P(A) = 0.1$ (length of interval $A$),
* $P(B) = 0.1$ (sum of lengths of intervals in $B$),
* $P(A \cap B) = 0.01$ (measure of overlap).

Thus,

$$
P(A \cap B) = 0.01 = 0.1 \cdot 0.1 = P(A)P(B),
$$

verifying independence. More generally, independence in continuous spaces can be rigorously justified using product measures and integration [ @Billingsley1995 ].

### Exercises

1. **Conditional Probability as a Random Variable**
   Let $X$ be a random variable on $(\Omega, \mathcal{A}, P)$ and $\mathcal{G} \subseteq \mathcal{A}$ a sub-$\sigma$-algebra. Prove that there exists a $\mathcal{G}$-measurable function $f$ such that, for any $G \in \mathcal{G}$, $\int_G X , dP = \int_G f , dP$.
   *(Hint: Radon–Nikodym theorem.)*

2. **Mutual vs. Pairwise Independence**
   Construct three events that are pairwise independent but not mutually independent. Prove the distinction explicitly.

