## Random Variables

### Definition and Properties

Let $(\Omega, \mathcal{F}, P)$ be a probability space, where $\Omega$ is the sample space, $\mathcal{F}$ is a $\sigma$-algebra of subsets of $\Omega$, and $P$ is a probability measure on $\mathcal{F}$.
A **random variable** is a measurable function $X: \Omega \to \mathbb{R}$, meaning for all $x \in \mathbb{R}$, the set ${\omega \in \Omega : X(\omega) \leq x}$ belongs to $\mathcal{F}$ [@Billingsley1995].

The **cumulative distribution function (CDF)** of $X$ is defined by

$$
F_X(x) = P(X \leq x), \quad x \in \mathbb{R}.
$$

#### Properties of the CDF

1. $F_X$ is monotonically non-decreasing: If $x_1 < x_2$, then $F_X(x_1) \leq F_X(x_2)$.
2. $F_X$ is right-continuous: $\lim_{h \to 0^+} F_X(x+h) = F_X(x)$ for all $x$.
3. Limits: $\lim_{x \to -\infty} F_X(x) = 0$ and $\lim_{x \to \infty} F_X(x) = 1$.

**Proof**:
Monotonicity and right-continuity follow directly from the definition of the probability measure and the properties of $\sigma$-algebras [@Billingsley1995, p.14].
Limits at $\pm\infty$ follow from the countable additivity of $P$ and the exhaustion of the real line.

### Discrete Random Variables

A random variable $X$ is **discrete** if its range is a finite or countable set $S \subset \mathbb{R}$. The **probability mass function (pmf)** is

$$
p_X(x) = P(X = x), \quad x \in S
$$

with $\sum_{x \in S} p_X(x) = 1$.

#### Example: Sum of Two Fair Dice

Let $X$ be the sum of outcomes from rolling two independent fair dice. The possible values are $x \in {2, 3, \dots, 12}$.
Each outcome $(i, j)$ with $i, j \in {1,\dots,6}$ is equally likely. The number of pairs $(i,j)$ yielding sum $x$ is $n(x) = 6 - |7 - x|$, so

$$
p_X(x) = \frac{n(x)}{36}, \quad n(x) = 6 - |7 - x|, \quad x \in {2,3,\dots,12}.
$$

**Derivation**: For $x=7$, $n(7)=6$; for $x=2$, $n(2)=1$; etc.

### Continuous Random Variables

A random variable $X$ is **continuous** if there exists a **probability density function (pdf)** $f_X:\mathbb{R}\to[0,\infty)$ such that, for all $a < b$,

$$
P(a < X \leq b) = \int_a^b f_X(x)\,dx.
$$

The pdf satisfies

1. $f_X(x) \geq 0$ for all $x \in \mathbb{R}$
2. $\int_{-\infty}^{\infty} f_X(x),dx = 1$

**Remark**: Existence of a pdf requires $F_X$ to be absolutely continuous [@Durrett2019, Sec. 1.3].

#### Example: Standard Normal Distribution

Let $X$ have the standard normal distribution:

$$
f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}, \quad x \in \mathbb{R}
$$

Then, for any $a < b$,

$$
P(a < X \leq b) = \int_a^b f_X(x)\,dx.
$$

### Transformations of Random Variables

Given a random variable $X$ with pdf $f_X$ and a measurable, invertible, and differentiable function $h:\mathbb{R}\to\mathbb{R}$, define $Y = h(X)$. The pdf of $Y$ is

$$
f_Y(y) = f_X(h^{-1}(y)) \left| \frac{d}{dy} h^{-1}(y) \right|, \quad y \in h(\mathbb{R})
$$

**Assumptions**: $h$ is strictly monotonic and differentiable with inverse $h^{-1}$, and $f_X$ is known.

#### Example: Squared Standard Normal

Let $X \sim N(0,1)$, $Y = X^2$. Then for $y \geq 0$,

* $h^{-1}(y) = \pm \sqrt{y}$,
* $f_X(h^{-1}(y)) = \frac{1}{\sqrt{2\pi}} e^{-y/2}$ for both roots,
* $\left|\frac{d}{dy} h^{-1}(y)\right| = \frac{1}{2\sqrt{y}}$.

Thus,

$$
f_Y(y) = f_X(\sqrt{y})\frac{1}{2\sqrt{y}} + f_X(-\sqrt{y})\frac{1}{2\sqrt{y}} = \frac{1}{\sqrt{2\pi y}} e^{-y/2}, \quad y > 0,
$$

which is the chi-square distribution with one degree of freedom [@Billingsley1995, Sec. 10].

### Random Vectors

A **random vector** $\mathbf{X} = (X_1, \dots, X_n)$ is a measurable function $\mathbf{X}:\Omega \to \mathbb{R}^n$. The **joint distribution function** is

$$
F_{\mathbf{X}}(x_1, \dots, x_n) = P(X_1 \leq x_1, \dots, X_n \leq x_n)
$$

If a joint pdf $f_{\mathbf{X}}$ exists, then

$$
P((X_1, \dots, X_n) \in B) = \int_{B} f_{\mathbf{X}}(x_1, \dots, x_n)\,dx_1 \cdots dx_n
$$

for all Borel sets $B \subset \mathbb{R}^n$.

**Marginals and Conditionals**: The marginal pdf of $X_1$ is

$$
f_{X_1}(x_1) = \int_{\mathbb{R}^{n-1}} f_{\mathbf{X}}(x_1, x_2, \dots, x_n)\,dx_2 \cdots dx_n
$$

Conditional distributions are defined analogously [@Durrett2019, Sec. 1.5].

#### Example: Bivariate Normal

Let $(X_1, X_2)$ be jointly normal with mean zero, variances $1$, and covariance $\rho$. The joint pdf is

$$
f_{\mathbf{X}}(x_1, x_2) = \frac{1}{2\pi\sqrt{1-\rho^2}} \exp\left( -\frac{x_1^2 - 2\rho x_1 x_2 + x_2^2}{2(1-\rho^2)} \right)
$$

### Independence of Random Variables

Random variables $X$ and $Y$ are **independent** if, for all $x,y \in \mathbb{R}$,

$$
P(X \leq x, Y \leq y) = P(X \leq x) P(Y \leq y)
$$

Equivalently, $F_{(X,Y)}(x,y) = F_X(x) F_Y(y)$ for all $x, y$.

**Equivalent Criteria**: Independence holds if and only if $P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$ for all Borel sets $A, B \subseteq \mathbb{R}$ [@Billingsley1995, Thm. 12.1].

**Counterexample**: Let $X$ be uniform on $[-1,1]$, $Y=X$. $P(X \leq x, Y \leq y) = P(X \leq \min{x, y})$ is not equal to $P(X \leq x)P(Y \leq y)$ unless $x$ or $y$ is $-\infty$ or $\infty$.

### Expectation and Variance

Let $X$ be a random variable.

* **Expectation**:

  * Discrete: $E[X] = \sum_{x} x p_X(x)$, provided $\sum_x |x| p_X(x) < \infty$.
  * Continuous: $E[X] = \int_{-\infty}^\infty x f_X(x),dx$ if $\int |x| f_X(x),dx < \infty$.

* **Variance**:

$$
\operatorname{Var}(X) = E\left[(X - E[X])^2\right] = E[X^2] - (E[X])^2
$$

**Properties**: For any $a,b \in \mathbb{R}$,

* $E[aX + b] = aE[X] + b$
* $\operatorname{Var}(aX + b) = a^2 \operatorname{Var}(X)$

**Proofs**: See [@Durrett2019, Prop. 1.5.1].

### Simulation of Random Variables

The **inverse transform method** is a fundamental simulation technique:
If $U \sim \text{Uniform}(0,1)$ and $F_X$ is continuous and strictly increasing, then $X = F_X^{-1}(U)$ has distribution function $F_X$.

**Advanced simulation**:

* **Acceptance-rejection method**: Generate $Y$ with density $g$, accept with probability $f_X(Y)/[c g(Y)]$, where $c$ satisfies $f_X(y) \leq c g(y)$ for all $y$ [@Devroye1986].
* **Boxâ€“Muller method**: For standard normal $X$, generate $U_1, U_2$ independent, uniform on $(0,1)$. Then $X = \sqrt{-2\log U_1} \cos(2\pi U_2)$.

### Exercises

1. Prove that for any CDF $F$, the function is right-continuous and satisfies the stated limits at $\pm\infty$.
2. Let $X$ be uniform on $[0,1]$, and $Y = -\ln(1-X)$. Show that $Y$ is exponential with parameter $1$.
3. Given random variables $X$ and $Y$ with joint pdf $f(x,y) = 6xy$ for $0 < x < 1$, $0 < y < 1$, $x+y < 1$, compute $P(X < Y)$ and check if $X$ and $Y$ are independent.

